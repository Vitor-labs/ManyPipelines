 <h1> <center> NHTSA ETL PIPELINE </center> </h1>

## Table of Content

- [Table of Content](#table-of-content)
- [How to run this project](#how-to-run-this-project)
  - [Explanation of the Project Structure](#explanation-of-the-project-structure)
  - [Pipeline Archtecture](#pipeline-archtecture)
  - [Cloud Archtecture:](#cloud-archtecture)
  - [See the Docs](#see-the-docs)

## How to run this project

**ðŸš¨ATENTION BE ALWAYS ON FORD VPN WHEN STALLING ENV PACKAGESðŸš¨**

1. Install the required dependencies:

```bash
conda create --name nhtsa-pipeline --file environment.yml
```

2. Activate the enviroment:
```bash
conda activate nhtsa-pipeline
```

3. Have sure that pre-commit is intalled:

```bash
pre-commit install
```

### Explanation of the Project Structure

<pre>
ðŸ“¦
â”œâ”€ ðŸ“œLICENSE
â”œâ”€ ðŸ“œREADME.md          <- This file.
â”œâ”€ âš™ï¸.gitignore         <- git configuration.
â”œâ”€ ðŸ“‚.github            <- GitHub Actions definitions
â”œâ”€ ðŸ“‚data
â”‚   â”œâ”€ ðŸ“‚external       <- Data from third party sources.
â”‚   â”œâ”€ ðŸ“‚processed      <- The final, canonical data sets for modeling.
â”‚   â”œâ”€ ðŸ“‚raw            <- The original, immutable data dump and system logs.
â”‚   â””â”€ ðŸ“œreferences.md  <- Data dictionaries, manuals, and all other explanatory materials.
â”‚
â”œâ”€ ðŸ“‚frontend           <- Form-Check-List to send after collecting and process data.
â”‚
â”œâ”€ ðŸ“‚notebooks          <- Jupyter notebooks. Naming convention is a number (for ordering), and
|   |                       a short `-` with a description, ex: `1.0-nhtsa_data_colector.ipynb`.
â”‚   â”‚
â”‚   â””â”€ ðŸ“‚scripts        <- Notebooks notebooks turned into scripts, some individual scripts and more
â”‚
â”œâ”€ ðŸ“œenvironment.yml    <- The requirements file for reproducing the analysis environment, e.g.
â”‚                           generated with `conda list -e > environment.yml`
â”‚
â””â”€ ðŸ“‚src                <- Source code for use in this project.
</pre>

### Pipeline Archtecture

<pre>
ðŸ“¦src
â”œâ”€ðŸ“œREADME.md           <- The top-level README for developers using this project.
â”œâ”€âš™ï¸.gitignore          <- git configuration.
â”œâ”€ðŸ“‚drivers
â”‚  â”œâ”€ðŸ“‚interfaces       <- Dependency Inversion.
â”‚  â”‚  â”œâ”€ðŸstructure_df  <- datasets types definitions & save CSV dataset
â”‚  â”‚  â””â”€ðŸhttp_request  <- Request Interface.
â”‚  â”‚
â”‚  â”œâ”€ðŸstructure_df     <- structure of dataset definiton for further add of columns
â”‚  â””â”€ðŸhttp_request     <- Request Inplementation.
â”‚
â”œâ”€ðŸ“‚errors              <- Errors and Exeptions Definitions.
â”‚  â”œâ”€ðŸextract_error    <- Errors definitions for extraction step
â”‚  â”œâ”€ðŸtransform_error  <- Errors definitions for Transformation step
â”‚  â””â”€ðŸload_error       <- Errors definitions for Loading step
â”‚  
â”œâ”€ðŸ“‚infra               <- Code Infrastructure (Database management, 3Âº parties
â”‚  â”‚                       connections, dataflow visualization, etc).
â”‚  â””â”€ðŸ“‚interfaces       <- Dependency Inversion.
â”‚
â”œâ”€ðŸ“‚main                <- Pipeline Control flow.
â”‚  â””â”€ðŸpipeline         <- defines the flow of the ETL process 'run.py'.
â”‚  â””â”€ðŸmain             <- file alike the common 'run.py'.
â”‚  
â””â”€ðŸ“‚stages              <- Definition for each stage and contracts for data transmition.
   â”œâ”€ðŸ“‚contracts        <- Defines contracts for data transmition.
   â”œâ”€ðŸ“‚utils            <- Some helper functions with decorators and loggers.
   â”œâ”€ðŸ“‚extract          <- Defines Extract Data Step.
   â”œâ”€ðŸ“‚transform        <- Defines Transform Data Step.
   â””â”€ðŸ“‚load             <- Defines Load Data Step.
</pre>

### Cloud Archtecture:
![GCP](./reports/diagram-export-1-5-2024-12_29_33-PM.svg)

### See the Docs

- **<https://app.eraser.io/workspace/my8RA5wATDvoaEuLOffg?origin=share>**
